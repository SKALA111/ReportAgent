import os
import pandas as pd
import openai
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ëª¨ë¸ ì´ˆê¸°í™”
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
model = SentenceTransformer("all-MiniLM-L6-v2")
embedding_dim = 384

# íŒŒì¼ ê²½ë¡œ
CSV_FILE_PATH = "/workspaces/ReportAgent/data/startups.csv"

# ë²¡í„° ì €ì¥ì†Œ
company_texts = []
company_names = []
company_embeddings = []

# ê¸°ì—… ì •ë³´ ë¬¸ì„œí™”
def format_company_doc(row):
    return f"""
    [ê¸°ì—…ëª…]: {row['ìŠ¤íƒ€íŠ¸ì—…']}
    [ë¶„ì•¼]: {row['ë¶„ì•¼']}
    [íŠ¹ì§•]: {row['íŠ¹ì§•']}
    [ì„±ì¥ í¬ì¸íŠ¸]: {row['ì„±ì¥ í¬ì¸íŠ¸']}
    """

# RAG ê¸°ë°˜ ë¶„ì„
def rag_analysis_for_company(company_idx, data, index):
    target_doc = company_texts[company_idx]
    target_emb = company_embeddings[company_idx]

    # ìœ ì‚¬ ê¸°ì—… íƒìƒ‰
    D, I = index.search(np.array([target_emb]), 4)
    context_docs = [company_texts[i] for i in I[0] if i != company_idx]
    context = "\n\n".join(context_docs)

    prompt = f"""
    ì£¼ìš” ìŠ¤íƒ€íŠ¸ì—…ë“¤ ì •ë³´:\n\n{context}\n\n
    ëŒ€ìƒ ê¸°ì—…:\n{target_doc}\n\n
    ì´ ê¸°ì—…ì˜ ì¬ë¬´ ìƒíƒœ, ì‹¤ì , í–¥í›„ ì„±ì¥ ê°€ëŠ¥ì„±ì„ ë‹¤ë¥¸ ê¸°ì—…ë“¤ê³¼ ë¹„êµí•˜ë©° ì „ë¬¸ê°€ì²˜ëŸ¼ ë¶„ì„í•´ì¤˜.
    """

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a startup investment analyst."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=700
    )
    return response.choices[0].message["content"].strip()

# ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸
def main():
    data = pd.read_csv(CSV_FILE_PATH)

    # Step 1: ê¸°ì—… ì •ë³´ ì„ë² ë”©
    for _, row in data.iterrows():
        doc = format_company_doc(row)
        emb = model.encode(doc)
        company_texts.append(doc)
        company_names.append(row["ìŠ¤íƒ€íŠ¸ì—…"])
        company_embeddings.append(emb)

    # Step 2: FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
    index = faiss.IndexFlatL2(embedding_dim)
    index.add(np.array(company_embeddings))

    # Step 3: ê° ê¸°ì—…ì— ëŒ€í•´ RAG ë¶„ì„
    analysis_results = []
    for i, name in enumerate(company_names):
        print(f"ğŸ” [{i+1}/{len(company_names)}] {name} ë¶„ì„ ì¤‘...")
        analysis = rag_analysis_for_company(i, data, index)
        analysis_results.append((name, analysis))

    # Step 4: ê²°ê³¼ ì €ì¥
    analysis_texts = [f"{name}: {analysis}" for name, analysis in analysis_results]
    analysis_embeddings = model.encode(analysis_texts)
    analysis_index = faiss.IndexFlatL2(embedding_dim)
    analysis_index.add(np.array(analysis_embeddings))

    faiss.write_index(analysis_index, "company_analysis.faiss")
    with open("company_analysis.pkl", "wb") as f:
        pickle.dump((analysis_texts, [name for name, _ in analysis_results]), f)

    print("âœ… ì „ì²´ ê¸°ì—… ë¶„ì„ ë° ë²¡í„° DB ì €ì¥ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
